<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Rishab Mudliar</title>
    <link>https://lazyCodes7.github.io/blogs/</link>
    <description>Recent content in Blogs on Rishab Mudliar</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <copyright>Copyright © 2026, Rishab Mudliar.</copyright>
    <lastBuildDate>Sat, 17 Feb 2024 12:09:56 +0530</lastBuildDate><atom:link href="https://lazyCodes7.github.io/blogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CS236 Deep Generative Models (Part6)</title>
      <link>https://lazyCodes7.github.io/blogs/cs236_6/</link>
      <pubDate>Sat, 17 Feb 2024 12:09:56 +0530</pubDate>
      
      <guid>https://lazyCodes7.github.io/blogs/cs236_6/</guid>
      <description>About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. A variational approximation to the posterior For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&amp;rsquo;s top half unknown this is what we came up with the last time</description>
    </item>
    
    <item>
      <title>CS236 Deep Generative Models (Part5)</title>
      <link>https://lazyCodes7.github.io/blogs/cs236_5/</link>
      <pubDate>Sun, 11 Feb 2024 12:48:56 +0530</pubDate>
      
      <guid>https://lazyCodes7.github.io/blogs/cs236_5/</guid>
      <description>About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Recap  Autoregressive models:  Chain rule based factorization is fully general Compact representation via conditional independence and/or neural parameterizations   Autoregressive models   Pros:
 Easy to evaluate likelihoods Easy to train    Cons:
 Requires an ordering Generation is sequential Cannot learn features in an unsupervised way      Latent Variable Models: Introduction   Lots of variability in images x due to gender, eye color, hair color, pose, etc.</description>
    </item>
    
    <item>
      <title>CS236 Deep Generative Models (Part4)</title>
      <link>https://lazyCodes7.github.io/blogs/cs236_4/</link>
      <pubDate>Mon, 05 Feb 2024 11:27:56 +0530</pubDate>
      
      <guid>https://lazyCodes7.github.io/blogs/cs236_4/</guid>
      <description>About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Recap Learning a generative model.   We are given a training set of examples, e.g., images of dogs
  We want to learn a probability distribution p(x) over images x such that
 Generation: If we sample xnew ∼ p(x), xnew should look like a dog (sampling) Density estimation: p(x) should be high if x looks like a dog, and low otherwise (anomaly detection) Unsupervised representation learning: We should be able to learn what these images have in common, e.</description>
    </item>
    
    <item>
      <title>CS236 Deep Generative Models (Part3)</title>
      <link>https://lazyCodes7.github.io/blogs/cs236_3/</link>
      <pubDate>Fri, 26 Jan 2024 15:27:56 +0530</pubDate>
      
      <guid>https://lazyCodes7.github.io/blogs/cs236_3/</guid>
      <description>About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Recap: Learning a generative model Recall that we want to learn a probability distribution p(x) over images x such that sampling from this distribution gives us new images. In the last part we dived deeper into probability distributions. Finally we saw ways to learn a probability distrubution for a discriminative model using techniques like logistic regression or neural models.</description>
    </item>
    
    <item>
      <title>CS236 Deep Generative Models (Part2)</title>
      <link>https://lazyCodes7.github.io/blogs/cs236_2/</link>
      <pubDate>Wed, 24 Jan 2024 15:27:56 +0530</pubDate>
      
      <guid>https://lazyCodes7.github.io/blogs/cs236_2/</guid>
      <description>About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Part 2 Learning a generative model In the last part we defined a statistical generative model represented as p(x) that lets us sample from this distribution to generate new images. But p(x) is unknown.
How do we learn it?
Before I go into it, we would be going into a lot of probablity terminologies that we will see.</description>
    </item>
    
    <item>
      <title>CS236 Deep Generative Models (Part1)</title>
      <link>https://lazyCodes7.github.io/blogs/cs236_1/</link>
      <pubDate>Fri, 19 Jan 2024 15:27:56 +0530</pubDate>
      
      <guid>https://lazyCodes7.github.io/blogs/cs236_1/</guid>
      <description>About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Part 1 Generative Modelling Generative modelling is a way to model/sample data that looks like something we might know.
I am going to put more weight on the last part as it is important to generate something that makes sense. For instance, a machine learning model that generates &amp;lsquo;qweefejfww&amp;rsquo; (random jargon) will not be of any use to us.</description>
    </item>
    
  </channel>
</rss>
