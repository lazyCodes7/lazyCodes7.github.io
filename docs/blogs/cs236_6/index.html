<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://lazyCodes7.github.io/images/favicon.png" />
<title>CS236 Deep Generative Models (Part6) | Rishab Mudliar</title>
<meta name="title" content="CS236 Deep Generative Models (Part6)" />
<meta name="description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. A variational approximation to the posterior For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time" />
<meta name="keywords" content="" />


<meta property="og:title" content="CS236 Deep Generative Models (Part6)" />
<meta property="og:description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. A variational approximation to the posterior For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazyCodes7.github.io/blogs/cs236_6/" /><meta property="og:image" content="https://lazyCodes7.github.io/images/share.png"/><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2024-02-17T12:09:56+05:30" />
<meta property="article:modified_time" content="2024-02-17T12:09:56+05:30" /><meta property="og:site_name" content="Rishab M" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lazyCodes7.github.io/images/share.png"/>

<meta name="twitter:title" content="CS236 Deep Generative Models (Part6)"/>
<meta name="twitter:description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. A variational approximation to the posterior For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time"/>



<meta itemprop="name" content="CS236 Deep Generative Models (Part6)">
<meta itemprop="description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. A variational approximation to the posterior For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time"><meta itemprop="datePublished" content="2024-02-17T12:09:56+05:30" />
<meta itemprop="dateModified" content="2024-02-17T12:09:56+05:30" />
<meta itemprop="wordCount" content="1078"><meta itemprop="image" content="https://lazyCodes7.github.io/images/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Rishab Mudliar</h2>
</a>
<nav>
<a href="/blogs/">Blogs</a>

<a href="/posts/">GSoC</a>

<a href="/archive/">Archive</a>

<a href="/projects/">Projects</a>

</nav>
</header>
  <main>

<content>
  <h1 id="about">About</h1>
<p>These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.</p>
<h2 id="continuing">Continuing..</h2>
<h3 id="a-variational-approximation-to-the-posterior">A variational approximation to the posterior</h3>
<p><img src="/application51.png" alt="alt text"></p>
<p>For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)</p>
<p>In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time</p>
<ul>
<li>We assumed p(z, x; θ) is close to pdata (z, x). z denotes the top half of the image
(assumed to be latent)</li>
<li>And theorized q(xtop ; ϕ) a (tractable) probability distribution over the hidden
variables (missing pixels in this example) xtop parameterized by ϕ
(variational parameters)</li>
</ul>
<p><img src="/application71.png" alt="alt text"></p>
<p>Given the probablity distribution we deduced that the last choice is probably the best one</p>
<ul>
<li>Is ϕi = 0.5 ∀i a good approximation to the posterior p(xtop |xbottom ; θ)? No</li>
<li>Is ϕi = 1 ∀i a good approximation to the posterior p(xtop |xbottom ; θ)? No</li>
<li>Is ϕi ≈ 1 for pixels i corresponding to the top part of digit 9 a good
approximation? Yes</li>
</ul>
<h3 id="learning-via-stochastic-variational-inference-svi">Learning via stochastic variational inference (SVI)</h3>
<p>Our goal is to optimize ELBO given below.</p>
<p><img src="/application73.png" alt="alt text"></p>
<p>Steps:</p>
<ul>
<li>Initialize θ, ϕ1 , · · · , ϕM</li>
<li>Randomly sample a data point xi from D</li>
<li>Optimize L(xi ; θ, ϕi ) as a function of ϕi :
<ul>
<li>Repeat ϕi = ϕi + η∇ϕi L(xi ; θ, ϕi )</li>
<li>until convergence to ϕi,∗ ≈ arg maxϕ L(xi ; θ, ϕ)</li>
</ul>
</li>
<li>Compute ∇θ L(xi ; θ, ϕi,∗ )</li>
<li>Update θ in the gradient direction. Go to step 2</li>
</ul>
<p>How to compute the gradients? There might not be a closed form solution for the expectations. So we use Monte Carlo sampling</p>
<p>To evaluate the bound, sample z1 , · · · , zK from q(z; ϕ) and estimate</p>
<p><img src="/application74.png" alt="alt text"></p>
<p>Calculate gradients w.r.t θ and ϕ on the Monte-Carlo estimate.</p>
<p>Key assumption: q(z; ϕ) is tractable, i.e., easy to sample from and evaluate</p>
<p>The gradient with respect to θ is easy</p>
<p><img src="/application75.png" alt="alt text"></p>
<p>The gradient with respect to ϕ is more complicated because the expectation
depends on ϕ</p>
<p>We still want to estimate with a Monte Carlo average</p>
<h3 id="reparametrization">Reparametrization</h3>
<p>Want to compute a gradient with respect to ϕ of</p>
<p><img src="/application76.png" alt="alt text"></p>
<p>where z is now continuous</p>
<p>Suppose q(z; ϕ) = N (µ, σ 2 I ) is Gaussian with parameters ϕ = (µ, σ). These
are equivalent ways of sampling</p>
<ul>
<li>Sample z ∼ q(z; ϕ)</li>
<li>Sample ϵ ∼ N (0, I ), z = µ + σϵ = g (ϵ; ϕ). g is deterministic!</li>
</ul>
<p>Using this equivalence we compute the expectation in two ways:</p>
<p><img src="/application77.png" alt="alt text"></p>
<p>Easy to estimate via Monte Carlo if r and g are differentiable w.r.t. ϕ and ϵ
is easy to sample from (backpropagation)</p>
<h4 id="using-reparametrization-for-elbo">Using Reparametrization for ELBO</h4>
<p><img src="/application78.png" alt="alt text">
Using the reparameterization trick in ELBO. We can rewrite our equation as Eq(z;ϕ) [r (z, ϕ)] but if we observe it is not the same as Eq(z;ϕ) [r (z)] as our inner value in expectation depends on ϕ</p>
<p>Does this change this? Yes
Can we still use reparametrization? Yes</p>
<p>Assume z = µ + σϵ = g (ϵ; ϕ) like before.
Then</p>
<p><img src="/application79.png" alt="alt text"></p>
<h3 id="amortized-inference">Amortized Inference</h3>
<p><img src="/application80.png" alt="alt text"></p>
<ul>
<li>So far we have used a set of variational parameters ϕi for each data point xi . Does not scale to large datasets.</li>
<li>Amortization: Now we learn a single parametric function fλ that maps each x to a set of (good) variational parameters. Like doing regression on xi → ϕi,∗
<ul>
<li>For example, if q(z|xi ) are Gaussians with different means µ1 , · · · , µm , we learn a single neural network fλ mapping xi to µi</li>
</ul>
</li>
<li>We approximate the posteriors q(z|xi ) using this distribution qλ (z|x)</li>
</ul>
<h4 id="once-again">Once again&hellip;</h4>
<p><img src="/application81.png" alt="alt text"></p>
<ul>
<li>Assume p(z, xi ; θ) is close to pdata (z, xi ). Suppose z captures information
such as the digit identity (label), style, etc.</li>
<li>q(z; ϕi ) is a (tractable) probability distribution over the hidden
variables z parameterized by ϕi
(variational parameters)</li>
<li>For each xi , need to find a good ϕi,∗ (via optimization, expensive).</li>
<li><strong>Amortized inference:</strong> learn how to map xi to a good set of parameters ϕi
via q(z; fλ (xi )). fλ learns how to solve the optimization problem for you</li>
<li>For simplicity we refer q(z; fλ (xi)) as qϕ (z|x)</li>
</ul>
<h4 id="learning-with-amortized-inference">Learning with Amortized Inference</h4>
<p>As before, optimize ELBO as a function of θ, ϕ using (stochastic)
gradient descent</p>
<p><img src="/application82.png" alt="alt text"></p>
<ul>
<li>Initialize θ(0) , ϕ(0)</li>
<li>Randomly sample a data point xi from D</li>
<li>Compute ∇θ L(xi ; θ, ϕ) and ∇ϕ L(xi ; θ, ϕ)</li>
<li>Update θ, ϕ in the gradient direction</li>
<li>How to compute the gradients? Use reparameterization like before</li>
</ul>
<h3 id="autoencoder-perspective">Autoencoder: Perspective</h3>
<p><img src="/application83.png" alt="alt text"></p>
<ul>
<li>Take a data point xi , map it to ẑ by sampling from qϕ (z|xi ) (encoder).
Sample from a Gaussian with parameters (µ, σ) = encoderϕ (xi )</li>
<li>Reconstruct x̂ by sampling from p(x|ẑ; θ) (decoder). Sample from a
Gaussian with parameters decoderθ (ẑ)</li>
</ul>
<p>What does the training objective L(x; θ, ϕ) do?</p>
<ul>
<li>First term encourages x̂ ≈ xi (xi likely under p(x|ẑ; θ)). Autoencoding loss!</li>
<li>Second term encourages ẑ to have a distribution similar to the prior p(z)\</li>
</ul>
<p>Intution behind the two terms?</p>
<ul>
<li>The first part is supposed to help us give likely completions given z. Or in terms of the perspective of the decoder it means that given x̂ from encoder we should be able to reconstruct xi</li>
<li>Second term encourages ẑ to have a distribution similar to the prior p(z)</li>
</ul>
<h4 id="continued">Continued</h4>
<p><img src="/application84.png" alt="alt text"></p>
<ul>
<li>
<p>Alice goes on a space mission and needs to send images to Bob.
Given an image xi , she (stochastically) compresses it using
ẑ ∼ qϕ (z|xi ) obtaining a message ẑ. Alice sends the message ẑ to Bob</p>
</li>
<li>
<p>Given ẑ, Bob tries to reconstruct the image using p(x|ẑ; θ)</p>
<ul>
<li>This scheme works well if Eqϕ (z|x) [log p(x|z; θ)] is large</li>
<li>The term DKL (qϕ (z|x)∥p(z)) forces the distribution over messages to have a specific shape p(z). If Bob knows p(z), he can generate realistic messages ẑ ∼ p(z) and the corresponding image, as if he had received them from Alice!</li>
</ul>
</li>
</ul>
<h3 id="summary-of-latent-variable-models">Summary of Latent Variable Models</h3>
<ul>
<li>Combine simple models to get a more flexible one (e.g., mixture of
Gaussians)</li>
<li>Directed model permits ancestral sampling (efficient generation):
z ∼ p(z), x ∼ p(x|z; θ)</li>
<li>However, log-likelihood is generally intractable, hence learning is
difficult</li>
<li>Joint learning of a model (θ) and an amortized inference component
(ϕ) to achieve tractability via ELBO optimization</li>
<li>Latent representations for any x can be inferred via qϕ (z|x)</li>
</ul>

</content>



<p>
  
</p>

  </main>
  <footer>Made with <a href="https://github.com/janraasch/hugo-bearblog/">Hugo ʕ•ᴥ•ʔ Bear</a>
</footer>

  
</body>

</html>
