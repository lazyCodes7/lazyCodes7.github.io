<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  CS236 Deep Generative Models (Part6) · rishab_m
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">


<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;">




<meta name="author" content="Rishab Mudliar">
<meta name="description" content="About Link to heading These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. Link to heading A variational approximation to the posterior Link to heading For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time">
<meta name="keywords" content="blog,developer,personal">

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="CS236 Deep Generative Models (Part6)"/>
<meta name="twitter:description" content="About Link to heading These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. Link to heading A variational approximation to the posterior Link to heading For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time"/>

<meta property="og:title" content="CS236 Deep Generative Models (Part6)" />
<meta property="og:description" content="About Link to heading These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Continuing.. Link to heading A variational approximation to the posterior Link to heading For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)
In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazycodes7.github.io/blogs/cs236_6/" /><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2024-02-17T12:09:56+05:30" />
<meta property="article:modified_time" content="2024-02-17T12:09:56+05:30" />





<link rel="canonical" href="https://lazycodes7.github.io/blogs/cs236_6/">


<link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.e1bdf152d93b060b06ba5d496486ed9c201a8b95d335e035beb5faebe3b61cad.css" integrity="sha256-4b3xUtk7BgsGul1JZIbtnCAai5XTNeA1vrX66&#43;O2HK0=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/rm.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      rishab_m
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/blogs/">Blogs</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">GSoC</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/projects/">Projects</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container page">
  <article>
    <header>
      <h1 class="title">
        <a class="title-link" href="https://lazycodes7.github.io/blogs/cs236_6/">
          CS236 Deep Generative Models (Part6)
        </a>
      </h1>
    </header>

    <h1 id="about">
  About
  <a class="heading-link" href="#about">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.</p>
<h2 id="continuing">
  Continuing..
  <a class="heading-link" href="#continuing">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="a-variational-approximation-to-the-posterior">
  A variational approximation to the posterior
  <a class="heading-link" href="#a-variational-approximation-to-the-posterior">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p><img src="/application51.png" alt="alt text"></p>
<p>For cases when p(z|x; θ) is intractable we come up with a tractable approximation q(z; ϕ) that is as close as possible to p(z|x; θ)</p>
<p>In the case of an image having it&rsquo;s top half unknown this is what we came up with the last time</p>
<ul>
<li>We assumed p(z, x; θ) is close to pdata (z, x). z denotes the top half of the image
(assumed to be latent)</li>
<li>And theorized q(xtop ; ϕ) a (tractable) probability distribution over the hidden
variables (missing pixels in this example) xtop parameterized by ϕ
(variational parameters)</li>
</ul>
<p><img src="/application71.png" alt="alt text"></p>
<p>Given the probablity distribution we deduced that the last choice is probably the best one</p>
<ul>
<li>Is ϕi = 0.5 ∀i a good approximation to the posterior p(xtop |xbottom ; θ)? No</li>
<li>Is ϕi = 1 ∀i a good approximation to the posterior p(xtop |xbottom ; θ)? No</li>
<li>Is ϕi ≈ 1 for pixels i corresponding to the top part of digit 9 a good
approximation? Yes</li>
</ul>
<h3 id="learning-via-stochastic-variational-inference-svi">
  Learning via stochastic variational inference (SVI)
  <a class="heading-link" href="#learning-via-stochastic-variational-inference-svi">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Our goal is to optimize ELBO given below.</p>
<p><img src="/application73.png" alt="alt text"></p>
<p>Steps:</p>
<ul>
<li>Initialize θ, ϕ1 , · · · , ϕM</li>
<li>Randomly sample a data point xi from D</li>
<li>Optimize L(xi ; θ, ϕi ) as a function of ϕi :
<ul>
<li>Repeat ϕi = ϕi + η∇ϕi L(xi ; θ, ϕi )</li>
<li>until convergence to ϕi,∗ ≈ arg maxϕ L(xi ; θ, ϕ)</li>
</ul>
</li>
<li>Compute ∇θ L(xi ; θ, ϕi,∗ )</li>
<li>Update θ in the gradient direction. Go to step 2</li>
</ul>
<p>How to compute the gradients? There might not be a closed form solution for the expectations. So we use Monte Carlo sampling</p>
<p>To evaluate the bound, sample z1 , · · · , zK from q(z; ϕ) and estimate</p>
<p><img src="/application74.png" alt="alt text"></p>
<p>Calculate gradients w.r.t θ and ϕ on the Monte-Carlo estimate.</p>
<p>Key assumption: q(z; ϕ) is tractable, i.e., easy to sample from and evaluate</p>
<p>The gradient with respect to θ is easy</p>
<p><img src="/application75.png" alt="alt text"></p>
<p>The gradient with respect to ϕ is more complicated because the expectation
depends on ϕ</p>
<p>We still want to estimate with a Monte Carlo average</p>
<h3 id="reparametrization">
  Reparametrization
  <a class="heading-link" href="#reparametrization">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Want to compute a gradient with respect to ϕ of</p>
<p><img src="/application76.png" alt="alt text"></p>
<p>where z is now continuous</p>
<p>Suppose q(z; ϕ) = N (µ, σ 2 I ) is Gaussian with parameters ϕ = (µ, σ). These
are equivalent ways of sampling</p>
<ul>
<li>Sample z ∼ q(z; ϕ)</li>
<li>Sample ϵ ∼ N (0, I ), z = µ + σϵ = g (ϵ; ϕ). g is deterministic!</li>
</ul>
<p>Using this equivalence we compute the expectation in two ways:</p>
<p><img src="/application77.png" alt="alt text"></p>
<p>Easy to estimate via Monte Carlo if r and g are differentiable w.r.t. ϕ and ϵ
is easy to sample from (backpropagation)</p>
<h4 id="using-reparametrization-for-elbo">
  Using Reparametrization for ELBO
  <a class="heading-link" href="#using-reparametrization-for-elbo">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p><img src="/application78.png" alt="alt text">
Using the reparameterization trick in ELBO. We can rewrite our equation as Eq(z;ϕ) [r (z, ϕ)] but if we observe it is not the same as Eq(z;ϕ) [r (z)] as our inner value in expectation depends on ϕ</p>
<p>Does this change this? Yes
Can we still use reparametrization? Yes</p>
<p>Assume z = µ + σϵ = g (ϵ; ϕ) like before.
Then</p>
<p><img src="/application79.png" alt="alt text"></p>
<h3 id="amortized-inference">
  Amortized Inference
  <a class="heading-link" href="#amortized-inference">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p><img src="/application80.png" alt="alt text"></p>
<ul>
<li>So far we have used a set of variational parameters ϕi for each data point xi . Does not scale to large datasets.</li>
<li>Amortization: Now we learn a single parametric function fλ that maps each x to a set of (good) variational parameters. Like doing regression on xi → ϕi,∗
<ul>
<li>For example, if q(z|xi ) are Gaussians with different means µ1 , · · · , µm , we learn a single neural network fλ mapping xi to µi</li>
</ul>
</li>
<li>We approximate the posteriors q(z|xi ) using this distribution qλ (z|x)</li>
</ul>
<h4 id="once-again">
  Once again&hellip;
  <a class="heading-link" href="#once-again">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p><img src="/application81.png" alt="alt text"></p>
<ul>
<li>Assume p(z, xi ; θ) is close to pdata (z, xi ). Suppose z captures information
such as the digit identity (label), style, etc.</li>
<li>q(z; ϕi ) is a (tractable) probability distribution over the hidden
variables z parameterized by ϕi
(variational parameters)</li>
<li>For each xi , need to find a good ϕi,∗ (via optimization, expensive).</li>
<li><strong>Amortized inference:</strong> learn how to map xi to a good set of parameters ϕi
via q(z; fλ (xi )). fλ learns how to solve the optimization problem for you</li>
<li>For simplicity we refer q(z; fλ (xi)) as qϕ (z|x)</li>
</ul>
<h4 id="learning-with-amortized-inference">
  Learning with Amortized Inference
  <a class="heading-link" href="#learning-with-amortized-inference">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>As before, optimize ELBO as a function of θ, ϕ using (stochastic)
gradient descent</p>
<p><img src="/application82.png" alt="alt text"></p>
<ul>
<li>Initialize θ(0) , ϕ(0)</li>
<li>Randomly sample a data point xi from D</li>
<li>Compute ∇θ L(xi ; θ, ϕ) and ∇ϕ L(xi ; θ, ϕ)</li>
<li>Update θ, ϕ in the gradient direction</li>
<li>How to compute the gradients? Use reparameterization like before</li>
</ul>
<h3 id="autoencoder-perspective">
  Autoencoder: Perspective
  <a class="heading-link" href="#autoencoder-perspective">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p><img src="/application83.png" alt="alt text"></p>
<ul>
<li>Take a data point xi , map it to ẑ by sampling from qϕ (z|xi ) (encoder).
Sample from a Gaussian with parameters (µ, σ) = encoderϕ (xi )</li>
<li>Reconstruct x̂ by sampling from p(x|ẑ; θ) (decoder). Sample from a
Gaussian with parameters decoderθ (ẑ)</li>
</ul>
<p>What does the training objective L(x; θ, ϕ) do?</p>
<ul>
<li>First term encourages x̂ ≈ xi (xi likely under p(x|ẑ; θ)). Autoencoding loss!</li>
<li>Second term encourages ẑ to have a distribution similar to the prior p(z)\</li>
</ul>
<p>Intution behind the two terms?</p>
<ul>
<li>The first part is supposed to help us give likely completions given z. Or in terms of the perspective of the decoder it means that given x̂ from encoder we should be able to reconstruct xi</li>
<li>Second term encourages ẑ to have a distribution similar to the prior p(z)</li>
</ul>
<h4 id="continued">
  Continued
  <a class="heading-link" href="#continued">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p><img src="/application84.png" alt="alt text"></p>
<ul>
<li>
<p>Alice goes on a space mission and needs to send images to Bob.
Given an image xi , she (stochastically) compresses it using
ẑ ∼ qϕ (z|xi ) obtaining a message ẑ. Alice sends the message ẑ to Bob</p>
</li>
<li>
<p>Given ẑ, Bob tries to reconstruct the image using p(x|ẑ; θ)</p>
<ul>
<li>This scheme works well if Eqϕ (z|x) [log p(x|z; θ)] is large</li>
<li>The term DKL (qϕ (z|x)∥p(z)) forces the distribution over messages to have a specific shape p(z). If Bob knows p(z), he can generate realistic messages ẑ ∼ p(z) and the corresponding image, as if he had received them from Alice!</li>
</ul>
</li>
</ul>
<h3 id="summary-of-latent-variable-models">
  Summary of Latent Variable Models
  <a class="heading-link" href="#summary-of-latent-variable-models">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ul>
<li>Combine simple models to get a more flexible one (e.g., mixture of
Gaussians)</li>
<li>Directed model permits ancestral sampling (efficient generation):
z ∼ p(z), x ∼ p(x|z; θ)</li>
<li>However, log-likelihood is generally intractable, hence learning is
difficult</li>
<li>Joint learning of a model (θ) and an amortized inference component
(ϕ) to achieve tractability via ELBO optimization</li>
<li>Latent representations for any x can be inferred via qϕ (z|x)</li>
</ul>

  </article>
</section>

  

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2019 -
    
    2024
     Rishab Mudliar 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
