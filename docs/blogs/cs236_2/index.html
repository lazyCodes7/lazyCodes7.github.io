<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://lazyCodes7.github.io/images/favicon.png" />
<title>CS236 Deep Generative Models (Part2) | Rishab Mudliar</title>
<meta name="title" content="CS236 Deep Generative Models (Part2)" />
<meta name="description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Part 2 Learning a generative model In the last part we defined a statistical generative model represented as p(x) that lets us sample from this distribution to generate new images. But p(x) is unknown.
How do we learn it?
Before I go into it, we would be going into a lot of probablity terminologies that we will see." />
<meta name="keywords" content="" />


<meta property="og:title" content="CS236 Deep Generative Models (Part2)" />
<meta property="og:description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Part 2 Learning a generative model In the last part we defined a statistical generative model represented as p(x) that lets us sample from this distribution to generate new images. But p(x) is unknown.
How do we learn it?
Before I go into it, we would be going into a lot of probablity terminologies that we will see." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazyCodes7.github.io/blogs/cs236_2/" /><meta property="og:image" content="https://lazyCodes7.github.io/images/share.png"/><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2024-01-24T15:27:56+05:30" />
<meta property="article:modified_time" content="2024-01-24T15:27:56+05:30" /><meta property="og:site_name" content="Rishab M" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lazyCodes7.github.io/images/share.png"/>

<meta name="twitter:title" content="CS236 Deep Generative Models (Part2)"/>
<meta name="twitter:description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Part 2 Learning a generative model In the last part we defined a statistical generative model represented as p(x) that lets us sample from this distribution to generate new images. But p(x) is unknown.
How do we learn it?
Before I go into it, we would be going into a lot of probablity terminologies that we will see."/>



<meta itemprop="name" content="CS236 Deep Generative Models (Part2)">
<meta itemprop="description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Part 2 Learning a generative model In the last part we defined a statistical generative model represented as p(x) that lets us sample from this distribution to generate new images. But p(x) is unknown.
How do we learn it?
Before I go into it, we would be going into a lot of probablity terminologies that we will see."><meta itemprop="datePublished" content="2024-01-24T15:27:56+05:30" />
<meta itemprop="dateModified" content="2024-01-24T15:27:56+05:30" />
<meta itemprop="wordCount" content="2138"><meta itemprop="image" content="https://lazyCodes7.github.io/images/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Rishab Mudliar</h2>
</a>
<nav>
<a href="/blogs/">Blogs</a>

<a href="/posts/">GSoC</a>

<a href="/projects/">Projects</a>

<a href="/archive/">Archive</a>

<a href="/cv/cv.pdf">CV</a>

</nav>
</header>
  <main>

<content>
  <h1 id="about">About</h1>
<p>These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.</p>
<h1 id="part-2">Part 2</h1>
<h1 id="learning-a-generative-model">Learning a generative model</h1>
<p>In the last part we defined a statistical generative model represented as p(x) that lets us sample from this distribution to generate new images. But p(x) is unknown.</p>
<p>How do we learn it?</p>
<p>Before I go into it, we would be going into a lot of probablity terminologies that we will see. So whoever is reading I hope you are good with your probability. If not, there are a lot of videos to understand these terms in an awesome way. I will try my best though and hope it is understandable</p>
<p>Alright let&rsquo;s get started then.</p>
<h2 id="probablity-distributions">Probablity Distributions</h2>
<p>Our goal is to learn a distribution that approximates the data that we are trying to generate. But what is a distribution? Let&rsquo;s understand with an example.</p>
<p>Take an unbiased coin and toss it. What is the output?
Write the probablity associated with it.
Now write the probablity associated with the event that did not occur</p>
<p>It should look something like P(X = YourOutput) = 1/2 P(X != YourOutput) = 1 - 1/2</p>
<p>Well now you basically have the recipe to create a distribution.</p>
<p>Here is the checklist.</p>
<ul>
<li>A set of random variables</li>
<li>The probablity associated with random distribution</li>
</ul>
<p>Once you satisfy the checklist you plot the probablities associated with the random variable. Congratulations on creating a distribution for the first time or not..</p>
<h2 id="types">Types</h2>
<h3 id="discrete-probability-distributions">Discrete Probability Distributions</h3>
<p>Probability distributions associated with random variables that are discrete in nature.</p>
<h4 id="examplestaken-from-the-slides">Examples(taken from the slides)</h4>
<h5 id="bernoulli-distribution-biased-coin-flip">Bernoulli distribution: (biased) coin flip</h5>
<p>D = {Heads, Tails} <br>
Specify P(X = Heads) = p. Then P(X = Tails) = 1 − p. <br>
Write: X ∼ Ber (p) <br>
Sampling: flip a (biased) coin \</p>
<h5 id="categorical-distribution-biased-m-sided-dice">Categorical distribution: (biased) m-sided dice</h5>
<p>D = {1, · · · , m} <br>
P <br>
Specify P(Y = i) = pi , such that <br>
pi = 1 <br>
Write: Y ∼ Cat(p1 , · · · , pm ) <br>
Sampling: roll a (biased) die</p>
<h3 id="continuous-probablity-distribution">Continuous Probablity Distribution</h3>
<p>A continuous probability distribution is one in which a continuous random variable X can take on any value within a given range of values ‒ which can be infinite, and therefore uncountable. For example, time is infinite because you could count from 0 to a billion seconds, a trillion seconds, and so on, forever. Source(<a href="https://www.knime.com/blog/learn-continuous-probability-distribution">https://www.knime.com/blog/learn-continuous-probability-distribution</a>)</p>
<p>Normally these distributions are defined using a probablity density function or PDFs not the file format:p</p>
<h4 id="example">Example</h4>
<h5 id="uniform-distribution">Uniform Distribution</h5>
<p>A Uniform distribution and is related to the events which are equally likely to occur. It is defined by two parameters, x and y, where x = minimum value and y = maximum value. It is generally denoted by u(x, y).</p>
<p>It is denoted as f(b)=1/y-x</p>
<h3 id="joint-probablity-distribution">Joint Probablity Distribution</h3>
<p>The next topic is joint distribution which can be thought of as a distribution consisting of two or more random variables.</p>
<h4 id="examples">Examples</h4>
<h5 id="flip-of-coins">Flip of coins</h5>
<p>Consider the flip of two fair coins; let A  and B  be discrete random variables associated with the outcomes of the first and second coin flips respectively. Each coin flip is a Bernoulli trial and has a Bernoulli distribution. If a coin displays &ldquo;heads&rdquo; then the associated random variable takes the value 1, and it takes the value 0 otherwise. The probability of each of these outcomes is 1/2.</p>
<p>To model the joint distribution we have to consider the all the possible cases.</p>
<p>In this case it will be</p>
<ul>
<li>P(A = Heads, B = Heads)</li>
<li>P(A = Heads, B = tails)</li>
<li>P(A = Tails, B = Heads)</li>
<li>P(A = Tails, B = Tails)</li>
</ul>
<p>Since the coins are fair the second coin won&rsquo;t have any dependence on the first coin. In that case we can denote P(A, B) as the product of it&rsquo;s probaility distribution</p>
<p>So each of the four cases are equally likely with a probablity of 1/4</p>
<p>and this joint distribution will be represented as P(A, B) = P(A)P(B) A,B -&gt; {Heads, Tails}</p>
<p>P(A) = 1/2 A -&gt; {Heads, Tails}</p>
<p>P(B) = 1/2 B -&gt; {Heads, Tails}</p>
<h3 id="sampling-from-a-distribution">Sampling from a distribution</h3>
<h4 id="modelling-a-pixels-color">Modelling a pixel&rsquo;s color</h4>
<p><img src="/application4.png" alt=""></p>
<p>Let&rsquo;s say we want to generate an image pixel by pixel in the RGB space.</p>
<p>For each channel assign a random variable. In this case -&gt; 3 random variables</p>
<p>Red Channel R. Val(R) = {0, · · · , 255} <br>
Green Channel G . Val(G) = {0, · · · , 255} <br>
Blue Channel B. Val(B) = {0, · · · , 255}</p>
<p>The joint distribution p(r, g, b) can be used to sample pixels.</p>
<p>But how many parameters would be required to represent all the possibilities in this distribution</p>
<p>Considering that each channel has 256 possible values. It will mean 256^3 possibilites and assuming that we assign the last probablity as 1 - the sum of possible probablities</p>
<p>Then we get the number of parameters as 256^3 - 1</p>
<h4 id="modelling-a-binary-image">Modelling a binary image</h4>
<p><img src="/application5.png" alt=""></p>
<p>Suppose X1 , . . . , Xn are binary (Bernoulli) random variables, i.e.,
Val(Xi ) = {0, 1} = {Black, White}.</p>
<p>Possible States?  Each variable takes two values 0 and 1 for n variable - 2^n combinations/states</p>
<p>Sampling from p(x1 , . . . , xn ) generates an image</p>
<p>No of parameters required to represent the distribution = 2^n-1</p>
<h3 id="approximating-distributions">Approximating distributions</h3>
<h4 id="structure-through-independence">Structure through Independence</h4>
<p>From previous examples we see that sampling an image can be expensive. Can we make some assumptions??</p>
<p>Taking the binary image example what if we assume that the pixels of the binary image are independent.</p>
<p>In that case,
p(x1 , . . . , xn ) = p(x1 )p(x2 ) · · · p(xn)</p>
<p>Possible states = Still 2^n</p>
<p>But parameters to represent this?
Assuming each bernoulli distribution can be represented by a single parameter px where px is probablity of a pixel being 1 and 1-px the opposite</p>
<p>In that case we can represent 2^n states by just using n random variables</p>
<p>(Side note: In the previous case that won&rsquo;t be possible as we don&rsquo;t have independence between variables and so the conditional probability would need to be represented by some parameter and hence it was 2^n-1)</p>
<p>Cons?</p>
<p><img src="/application6.png" alt=""></p>
<p>Independence assumption is too strong. Model not likely to be useful <br>
For example, each pixel chosen independently when we sample from it.</p>
<h4 id="structure-through-conditional-independence">Structure through conditional independence</h4>
<p>Using Chain Rule
p(x1 , . . . , xn ) = p(x1 )p(x2 | x1 )p(x3 | x1 , x2 ) · · · p(xn | x1 , · · · , xn−1 )</p>
<p>How many parameters? 1 + 2 + · · · + 2n−1 = 2n − 1 <br>
p(x1 ) requires 1 parameter <br>
p(x2 | x1 = 0) requires 1 parameter, p(x2 | x1 = 1) requires 1 parameter <br>
Total 2^n parameters.</p>
<p>2^n − 1 is still exponential, chain rule does not buy us anything.</p>
<p>Now suppose Xi+1 ⊥ X1 , . . . , Xi−1 | Xi <br>
What this means is that the variable Xi+1 is independent of variables from X1 to Xi-1 given Xi. In that case we can modify the p(x1,&hellip;.,xn) as</p>
<p>p(x1 , . . . , xn ) = p(x1 )p(x2 | x1 )p(x3 | x2 ) · · · p(xn | xn−1 )</p>
<p>This forms the idea for Bayes Networks.</p>
<h3 id="bayes-network-general-idea">Bayes Network: General Idea</h3>
<p>Use conditional parameterization (instead of joint parameterization)</p>
<p>For each random variable Xi , specify p(xi |xAi ) for set XAi of random variables</p>
<p>Then get joint parametrization as
p(x1 , . . . , xn ) = mult( p(xi |xAi ) ) for i = 1 to n</p>
<p>Need to guarantee it is a legal probability distribution. It has to
correspond to a chain rule factorization, with factors simplified due to
assumed conditional independencies</p>
<h3 id="bayes-networks-representation">Bayes Networks (Representation)</h3>
<p><img src="/application7.png" alt="">
A Bayesian network is specified by a directed acyclic graph (DAG)
G = (V , E ) with:</p>
<ol>
<li>One node i ∈ V for each random variable Xi</li>
<li>One conditional probability distribution (CPD) per node, p(xi | xPa(i) ), specifying the variable’s probability conditioned on its parents’ values
Graph G = (V , E ) is called the structure of the Bayesian Network</li>
</ol>
<p>Defines a joint distribution:
p(x1 , . . . xn ) = mult(p(xi | xPa(i) )) i∈V</p>
<p>Claim: p(x1 , . . . xn ) is a valid probability distribution because of ordering implied by DAG</p>
<p>Economical representation: exponential in |Pa(i)|, not |V |</p>
<h4 id="examples-1">Examples</h4>
<p><img src="/application8.png" alt=""></p>
<p>Consider the Bayes Network given.</p>
<p>How do we represent this network in terms of probablity?</p>
<p>For each node in the graph, consider the incoming edges those are the variables that the node depends on which also denotes the conditional independence on other variables given this incoming edges.</p>
<p>So, <br>
p(difficulty) = p(difficulty) <br>
p(intelligence) = p(intelligence) <br>
p(grade) = p(grade | difficulty, intelligence) <br>
p(letter) = p(letter | grade) <br>
p(sat) = p(sat | intelligence)</p>
<p>The joint distribution corresponding to the above BN factors as <br>
p(d, i, g , s, l) = p(d)p(i)p(g | i, d)p(s | i)p(l | g )</p>
<p>However, by the chain rule, any distribution can be written as <br>
p(d, i, g , s, l) = p(d)p(i | d)p(g | i, d)p(s | i, d, g )p(l | g , d, i, s)</p>
<p>As represented by the Bayesian network. We are assuming the conditional independence. <br>
D ⊥ I, <br>
S ⊥ {D, G } | I , <br>
L ⊥ {I , D, S} | G .</p>
<h5 id="spam-classification">Spam Classification</h5>
<p><img src="/application9.png" alt="">
Classify e-mails as spam (Y = 1) or not spam (Y = 0)</p>
<ul>
<li>Let 1 : n index the words in our vocabulary (e.g., English)</li>
<li>Xi = 1 if word i appears in an e-mail, and 0 otherwise</li>
<li>E-mails are drawn according to some distribution p(Y , X1 , . . . , Xn )</li>
</ul>
<p>Words are conditionally independent given Y</p>
<p>p(y , x1 , . . . xn ) = p(y) mult(p(xi | y )) for xi = 1 to n</p>
<p>To evaluate p(y=1 | x1,&hellip;. xn) use Bayes Rule</p>
<p><img src="/application10.png" alt=""></p>
<p>Are the independence assumptions made here reasonable? <br>
Philosophy: Nearly all probabilistic models are “wrong”, but many are
nonetheless useful</p>
<h3 id="discriminative-versus-generative-models">Discriminative versus generative models</h3>
<p><img src="/application11.png" alt=""></p>
<ul>
<li>Using chain rule p(Y , X) = p(X | Y )p(Y ) = p(Y | X)p(X). Corresponding Bayesian networks</li>
<li>However, suppose all we need for prediction is p(Y | X). In the left model, we need to specify/learn both p(Y ) and p(X | Y ), then compute p(Y | X) via Bayes rule</li>
<li>In the right model, it suffices to estimate just the conditional distribution p(Y | X)
<ul>
<li>We never need to model/learn/use p(X)!</li>
<li>Called a discriminative model because it is only useful for discriminating Y ’s label when given X</li>
</ul>
</li>
</ul>
<h4 id="example-1">Example</h4>
<h5 id="logistic-regression">Logistic Regression</h5>
<p>For the discriminative model, assume that p(Y = 1 | x; α) = f (x, α).</p>
<p>Not represented as a table anymore. It is a parameterized function of x (regression)</p>
<ul>
<li>Has to be between 0 and 1</li>
<li>Depend in some simple but reasonable way on x1 , · · · , xn</li>
<li>Completely specified by a vector α of n + 1 parameters (compact representation)</li>
</ul>
<p>Linear dependence: let z(α, x) = α0 + sum(αi*xi) for i = 1 to n .Then, p(Y = 1 | x; α) = σ(z(α, x)), where σ(z) = 1/(1 + e^−z ) is called the logistic function</p>
<h5 id="neural-models">Neural Models</h5>
<p>In discriminative models, we assume that <br>
p(Y = 1 | x; α) = f (x, α)</p>
<p>In previous example we have linear dependence</p>
<p>let z(α, x) = α0 + sum(αi*xi) for i = 1 to n <br>
p(Y = 1 | x; α) = σ(z(α, x)), where σ(z) = 1/(1 + e −z ) is the logistic function <br>
Dependence might be too simple</p>
<p>Solution? <br>
Non-linear dependence: let h(A, b, x) = f (Ax + b) be a non-linear transformation of the inputs (features).</p>
<p>pNeural (Y = 1 | x; α, A, b) = σ(α0 + sum(αi*hi) for i = 1 to n)</p>
<ul>
<li>More flexible</li>
<li>More parameters: A, b, α</li>
</ul>
<h3 id="bayes-networks-with-continuous-variables">Bayes Networks with Continuous Variables</h3>
<p>Bayes Networks can also be used in case of continuous Variables. Here is an example</p>
<h4 id="mixture-of-2-gaussians">Mixture of 2 Gaussians</h4>
<p>Consider that we want to map a bernoulli distribution Z to a gaussian distribution X. <br>
Need to find p(x|z) <br>
Z ∼ Bernoulli(p) <br>
p(x|z) = p(X | (Z = 0)) ∼ N (µ0 , σ0 ) , p(X | (Z = 1)) ∼ N (µ1 , σ1 ) <br>
The parameters are p, µ0 , σ0 , µ1 , σ1</p>
<p>By mixing 2 Gaussians we have applied the concept of Bayes Networks where X depends on the variable Z.</p>
<h3 id="wrapping-up">Wrapping up</h3>
<p>So this part we kind of got a lot into understanding about various probability distributions. How to sample from distributions, how to approximate them etc. Next part will pretty much focus on the same terminology but using a different class of models termed as autoregressive models. Until then bye!!</p>

</content>



<p>
  
</p>

  </main>
  <footer>
</footer>

  
</body>

</html>
