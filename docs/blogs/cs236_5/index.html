<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://lazyCodes7.github.io/images/favicon.png" />
<title>CS236 Deep Generative Models (Part5) | Rishab Mudliar</title>
<meta name="title" content="CS236 Deep Generative Models (Part5)" />
<meta name="description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Recap  Autoregressive models:  Chain rule based factorization is fully general Compact representation via conditional independence and/or neural parameterizations   Autoregressive models   Pros:
 Easy to evaluate likelihoods Easy to train    Cons:
 Requires an ordering Generation is sequential Cannot learn features in an unsupervised way      Latent Variable Models: Introduction   Lots of variability in images x due to gender, eye color, hair color, pose, etc." />
<meta name="keywords" content="" />


<meta property="og:title" content="CS236 Deep Generative Models (Part5)" />
<meta property="og:description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Recap  Autoregressive models:  Chain rule based factorization is fully general Compact representation via conditional independence and/or neural parameterizations   Autoregressive models   Pros:
 Easy to evaluate likelihoods Easy to train    Cons:
 Requires an ordering Generation is sequential Cannot learn features in an unsupervised way      Latent Variable Models: Introduction   Lots of variability in images x due to gender, eye color, hair color, pose, etc." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazyCodes7.github.io/blogs/cs236_5/" /><meta property="og:image" content="https://lazyCodes7.github.io/images/share.png"/><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2024-02-11T12:48:56+05:30" />
<meta property="article:modified_time" content="2024-02-11T12:48:56+05:30" /><meta property="og:site_name" content="Rishab M" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lazyCodes7.github.io/images/share.png"/>

<meta name="twitter:title" content="CS236 Deep Generative Models (Part5)"/>
<meta name="twitter:description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Recap  Autoregressive models:  Chain rule based factorization is fully general Compact representation via conditional independence and/or neural parameterizations   Autoregressive models   Pros:
 Easy to evaluate likelihoods Easy to train    Cons:
 Requires an ordering Generation is sequential Cannot learn features in an unsupervised way      Latent Variable Models: Introduction   Lots of variability in images x due to gender, eye color, hair color, pose, etc."/>



<meta itemprop="name" content="CS236 Deep Generative Models (Part5)">
<meta itemprop="description" content="About These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.
Recap  Autoregressive models:  Chain rule based factorization is fully general Compact representation via conditional independence and/or neural parameterizations   Autoregressive models   Pros:
 Easy to evaluate likelihoods Easy to train    Cons:
 Requires an ordering Generation is sequential Cannot learn features in an unsupervised way      Latent Variable Models: Introduction   Lots of variability in images x due to gender, eye color, hair color, pose, etc."><meta itemprop="datePublished" content="2024-02-11T12:48:56+05:30" />
<meta itemprop="dateModified" content="2024-02-11T12:48:56+05:30" />
<meta itemprop="wordCount" content="1320"><meta itemprop="image" content="https://lazyCodes7.github.io/images/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Rishab Mudliar</h2>
</a>
<nav>
<a href="/blogs/">Blogs</a>

<a href="/posts/">GSoC</a>

<a href="/archive/">Archive</a>

<a href="/projects/">Projects</a>

</nav>
</header>
  <main>

<content>
  <h1 id="about">About</h1>
<p>These blogs are my notes that represent my interpretation of the CS236 course taught by Stefano.</p>
<h2 id="recap">Recap</h2>
<ul>
<li>Autoregressive models:
<ul>
<li>Chain rule based factorization is fully general</li>
<li>Compact representation via conditional independence and/or neural parameterizations</li>
</ul>
</li>
<li>Autoregressive models
<ul>
<li>
<p>Pros:</p>
<ul>
<li>Easy to evaluate likelihoods</li>
<li>Easy to train</li>
</ul>
</li>
<li>
<p>Cons:</p>
<ul>
<li>Requires an ordering</li>
<li>Generation is sequential</li>
<li>Cannot learn features in an unsupervised way</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="latent-variable-models-introduction">Latent Variable Models: Introduction</h2>
<ul>
<li>
<p>Lots of variability in images x due to gender, eye color, hair color,
pose, etc. However, unless images are annotated, these factors of
variation are not explicitly available (latent).</p>
</li>
<li>
<p>Idea: explicitly model these factors using latent variables z</p>
</li>
</ul>
<h3 id="latent-variable-models-motivation">Latent Variable Models: Motivation</h3>
<p><img src="/application44.png" alt="alt text"></p>
<ul>
<li>Only shaded variables x are observed in the data (pixel values)</li>
<li>Latent variables z correspond to high level features
<ul>
<li>If z chosen properly, p(x|z) could be much simpler than p(x)</li>
<li>If we had trained this model, then we could identify features via p(z | x), e.g., p(EyeColor = Blue|x)</li>
</ul>
</li>
</ul>
<p>Challenge: Very difficult to specify these conditionals by hand</p>
<h3 id="deep-latent-variable-models">Deep Latent Variable Models</h3>
<p><img src="/application45.png" alt="alt text"></p>
<ul>
<li>Use neural networks to model the conditionals (deep latent variable models):
<ul>
<li>z ∼ N (0, I )</li>
<li>p(x | z) = N (µθ (z), Σθ (z)) where µθ ,Σθ are neural networks</li>
</ul>
</li>
<li>Hope that after training, z will correspond to meaningful latent
factors of variation (features). Unsupervised representation learning.</li>
<li>As before, features can be computed via p(z | x)</li>
</ul>
<h3 id="mixture-of-gaussians-a-shallow-latent-variable-model">Mixture of Gaussians: a Shallow Latent Variable Model</h3>
<ul>
<li>z ∼ Categorical(1, · · · , K )</li>
<li>p(x | z = k) = N (µk , Σk )</li>
</ul>
<p><img src="/application46.png" alt="alt text"></p>
<p>Generative process</p>
<ul>
<li>Pick a mixture component k by sampling z</li>
<li>Generate a data point by sampling from that Gaussian</li>
</ul>
<p><img src="/application47.png" alt="alt text"></p>
<p>By learning p(x|z) we can do clustering by using p(z|x) as shown in the above image.</p>
<p>Another advantage is that Gaussian distribution is a simple one. And might not model p(x|z) properly but by mixing multiple Gaussian models for each conditional we get a complex distribution that can approximate the true data distribution</p>
<p><img src="/application48.png" alt="alt text"></p>
<h3 id="variational-autoencoder">Variational Autoencoder</h3>
<p>Extending the concept of mixture of gaussians discussed in last section, a variational autoencoder can be thought of as a mixture of gaussians of infinite Gaussians</p>
<ul>
<li>z ∼ N (0, I )</li>
<li><img src="/application49.png" alt="alt text"></li>
<li>Even though p(x | z) is simple, the marginal p(x) is very
complex/flexible</li>
</ul>
<h3 id="marginal-likelihood">Marginal Likelihood</h3>
<p><img src="/application51.png" alt="alt text"></p>
<ul>
<li>Suppose some pixel values are missing at train time (e.g., top half)</li>
<li>Let X denote observed random variables, and Z the unobserved ones (also
called hidden or latent)</li>
<li>Suppose we have a model for the joint distribution (e.g., PixelCNN) = p(X, Z; θ)
What is the probability p(X = x̄; θ) of observing a training data point x̄?
<img src="/application50.png" alt="alt text"></li>
<li>Need to consider all possible ways to complete the image (fill green part)</li>
</ul>
<h4 id="marginal-likelihood-autoencoder">Marginal Likelihood (Autoencoder)</h4>
<p>A mixture of an infinite number of Gaussians:</p>
<ul>
<li>z ∼ N (0, I )</li>
<li>p(x | z) = N (µθ (z), Σθ (z)) where µθ ,Σθ are neural networks</li>
<li>Z are unobserved at train time (also called hidden or latent)</li>
<li>Suppose we have a model for the joint distribution. What is the probability p(X = x̄; θ) of observing a training data point x̄?
<img src="/application52.png" alt="alt text"></li>
</ul>
<h4 id="marginal-likelihood-continued">Marginal Likelihood Continued..</h4>
<ul>
<li>Considering the same joint distribution used to describe unobserved variables i.e <code>p(X, Z; θ)</code></li>
<li>We have a dataset D, where for each datapoint the X variables are observed (e.g., pixel values) and the variables Z are never observed (e.g., cluster or class id.) D = {x(1) , · · · , x(M) }.</li>
<li>Maximum likelihood learning:</li>
</ul>
<p><img src="/application53.png" alt="alt text"></p>
<ul>
<li><img src="/application54.png" alt="alt text"></li>
<li>Need approximations. One gradient evaluation per training data point
x ∈ D, so approximation needs to be cheap.</li>
</ul>
<h4 id="using-monte-carlo">Using Monte Carlo</h4>
<p>Likelihood function pθ (x) for Partially Observed Data is hard to compute:</p>
<p><img src="/application55.png" alt="alt text"></p>
<p>We can think of it as an (intractable) expectation. Monte Carlo to the rescue:</p>
<ul>
<li>Sample z(1) , · · · , z(k) uniformly at random</li>
<li>Approximate expectation with sample average</li>
</ul>
<p><img src="/application56.png" alt="alt text"></p>
<p>Works in theory but not in practice. For most z, pθ (x, z) is very low (most
completions don’t make sense). Some completions have large pθ (x, z) but we will
never ”hit” likely completions by uniform random sampling. Need a clever way to
select z(j) to reduce variance of the estimator.</p>
<p>What do we do? Could try to learn a way to sample z that is more likely to help us observe p(x)..</p>
<h4 id="importance-sampling">Importance Sampling</h4>
<p>Instead of using a z/z solution we introduce q(z) where q(z) is a probablity distribution from which we sample z that are more likely to complete x.</p>
<p><img src="/application57.png" alt="alt text"></p>
<p>Monte Carlo to the rescue:</p>
<ul>
<li>Sample z(1) , · · · , z(k) from q(z)</li>
<li>Approximate expectation with sample average
<img src="/application58.png" alt="alt text"></li>
</ul>
<h4 id="calculating-log-likelihood">Calculating log likelihood</h4>
<p>Likelihood function pθ (x) for Partially Observed Data is hard to compute:</p>
<p><img src="/application57.png" alt="alt text"></p>
<p>Monte Carlo to the rescue:</p>
<ul>
<li>Sample z(1) , · · · , z(k) from q(z)</li>
<li>Approximate expectation with sample average
<img src="/application58.png" alt="alt text"></li>
</ul>
<p>For training, we need the log-likelihood log (pθ (x)). We could estimate
it as:</p>
<p><img src="/application59.png" alt="alt text"></p>
<p>And for the same case of k=1 if we calculate the log of expectation without the monte carlo approximation. We see a potential problem</p>
<p><img src="/application60.png" alt="alt text"></p>
<p>On the right hand side is the log of the expectation of sampling a single z from q(z) and on the left hand side is the output of the monte carlo approximation. It is clear that both of the values are not the same thing for k=1. For a higher k it might be same but for the base case we see that our estimation is not good.</p>
<h3 id="evidence-lower-bound">Evidence Lower Bound</h3>
<p>The Log-Likehlihood for partially missing data is hard to compute.</p>
<p><img src="/application61.png" alt="alt text">
log() is a concave function. log(px + (1 − p)x′) ≥ p log(x) + (1 − p) log(x′).</p>
<p>Using Jensen Equality we approximate the Log Likelihood as given. It is called ELBO or Evidence Lower Bound where our goal is to maximize the ELBO as much as possible</p>
<p><img src="/application63.png" alt="alt text"></p>
<h4 id="variational-inference">Variational Inference</h4>
<p>ELBO is maximum or equal when q = p(z|x; θ)</p>
<p>Proof:
<img src="/application66.png" alt="alt text"></p>
<ul>
<li>Confirms our previous importance sampling intuition: we should
choose likely completions.</li>
</ul>
<h5 id="kl-divergence-relation-with-elbo">KL-Divergence relation with ELBO</h5>
<p>Suppose q(z) is any probability distribution over the hidden variables.
A little bit of algebra reveals</p>
<p><img src="/application67.png" alt="alt text"></p>
<p>Corresponding proof. <a href="https://chrisorm.github.io/VI-ELBO.html">External link</a></p>
<p><img src="/proof1.png" alt=""></p>
<p>Equality holds if q = p(z|x; θ) because DKL (q(z)∥p(z|x; θ))=0</p>
<p><img src="/application68.png" alt="alt text"></p>
<p>Then we can rewrite as follows.</p>
<p><img src="/application69.png" alt="alt text"></p>
<p>Which means -&gt; Minimizing KL-Divergence is equal to maximimzing ELBO</p>
<h4 id="computing-q">Computing q</h4>
<p><img src="/application70.png" alt="alt text"></p>
<ul>
<li>What if the posterior p(z|x; θ) is intractable to compute?</li>
<li>Suppose q(z; ϕ) is a (tractable) probability distribution over the hidden
variables parameterized by ϕ (variational parameters)
<ul>
<li>For example, a Gaussian with mean and covariance specified by ϕ
q(z; ϕ) = N (ϕ1 , ϕ2 )</li>
</ul>
</li>
<li>Variational inference: pick ϕ so that q(z; ϕ) is as close as possible to
p(z|x; θ). In the figure, the posterior p(z|x; θ) (blue) is better approximated
by N (2, 2) (orange) than N (−4, 0.75) (green)</li>
</ul>
<h5 id="variational-approximation-of-the-posterior">Variational approximation of the posterior</h5>
<p><img src="/application51.png" alt="alt text"></p>
<ul>
<li>
<p>Assume p(xtop , xbottom ; θ) assigns high probability to images that look like
digits. In this example, we assume z = xtop are unobserved (latent)</p>
</li>
<li>
<p>Suppose q(xtop ; ϕ) is a (tractable) probability distribution over the hidden
variables (missing pixels in this example) xtop parameterized by ϕ
(variational parameters)</p>
</li>
</ul>
<p><img src="/application71.png" alt="alt text"></p>
<ul>
<li>Is ϕi = 0.5 ∀i a good approximation to the posterior p(xtop |xbottom ; θ)? No</li>
<li>Is ϕi = 1 ∀i a good approximation to the posterior p(xtop |xbottom ; θ)? No</li>
<li>Is ϕi ≈ 1 for pixels i corresponding to the top part of digit 9 a good
approximation? Yes</li>
</ul>
<h4 id="closing-notes">Closing notes</h4>
<p><img src="/application72.png" alt="alt text"></p>
<p>The better q(z; ϕ) can approximate the posterior p(z|x; θ), the smaller
DKL (q(z; ϕ)∥p(z|x; θ)) we can achieve, the closer ELBO will be to
log p(x; θ). Next: jointly optimize over θ and ϕ to maximize the ELBO
over a dataset</p>
<ul>
<li>
<p>Latent Variable Models Pros:</p>
<ul>
<li>Easy to build flexible models</li>
<li>Suitable for unsupervised learning</li>
</ul>
</li>
<li>
<p>Latent Variable Models Cons:</p>
<ul>
<li>Hard to evaluate likelihoods</li>
<li>Hard to train via maximum-likelihood</li>
<li>Fundamentally, the challenge is that posterior inference p(z | x) is hard. Typically requires variational approximations</li>
</ul>
</li>
<li>
<p>Alternative: give up on KL-divergence and likelihood (GANs)</p>
</li>
</ul>

</content>



<p>
  
</p>

  </main>
  <footer>Made with <a href="https://github.com/janraasch/hugo-bearblog/">Hugo ʕ•ᴥ•ʔ Bear</a>
</footer>

  
</body>

</html>
