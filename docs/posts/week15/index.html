<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://lazyCodes7.github.io/images/favicon.png" />
<title>Week-15 (Coding Period) 25th August - 2nd September | Rishab Mudliar</title>
<meta name="title" content="Week-15 (Coding Period) 25th August - 2nd September" />
<meta name="description" content="This week  Implemented the transformer model for captioning task Presented the demo of the pipeline to the mentors  Changing the dataset(torch.Dataset instance i.e) Earlier the dataset instance of PyTorch that I was using was simple. Whenever I referred an index it would return the image and numericalized version of the caption. But I wanted to use GPT2 for what it is good for. Hint: Text generation. So I changed the captions by providing a prompt as a token." />
<meta name="keywords" content="" />


<meta property="og:title" content="Week-15 (Coding Period) 25th August - 2nd September" />
<meta property="og:description" content="This week  Implemented the transformer model for captioning task Presented the demo of the pipeline to the mentors  Changing the dataset(torch.Dataset instance i.e) Earlier the dataset instance of PyTorch that I was using was simple. Whenever I referred an index it would return the image and numericalized version of the caption. But I wanted to use GPT2 for what it is good for. Hint: Text generation. So I changed the captions by providing a prompt as a token." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazyCodes7.github.io/posts/week15/" /><meta property="og:image" content="https://lazyCodes7.github.io/images/share.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-02T23:56:36+05:30" />
<meta property="article:modified_time" content="2022-09-02T23:56:36+05:30" /><meta property="og:site_name" content="Rishab M" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lazyCodes7.github.io/images/share.png"/>

<meta name="twitter:title" content="Week-15 (Coding Period) 25th August - 2nd September"/>
<meta name="twitter:description" content="This week  Implemented the transformer model for captioning task Presented the demo of the pipeline to the mentors  Changing the dataset(torch.Dataset instance i.e) Earlier the dataset instance of PyTorch that I was using was simple. Whenever I referred an index it would return the image and numericalized version of the caption. But I wanted to use GPT2 for what it is good for. Hint: Text generation. So I changed the captions by providing a prompt as a token."/>



<meta itemprop="name" content="Week-15 (Coding Period) 25th August - 2nd September">
<meta itemprop="description" content="This week  Implemented the transformer model for captioning task Presented the demo of the pipeline to the mentors  Changing the dataset(torch.Dataset instance i.e) Earlier the dataset instance of PyTorch that I was using was simple. Whenever I referred an index it would return the image and numericalized version of the caption. But I wanted to use GPT2 for what it is good for. Hint: Text generation. So I changed the captions by providing a prompt as a token."><meta itemprop="datePublished" content="2022-09-02T23:56:36+05:30" />
<meta itemprop="dateModified" content="2022-09-02T23:56:36+05:30" />
<meta itemprop="wordCount" content="560"><meta itemprop="image" content="https://lazyCodes7.github.io/images/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Rishab Mudliar</h2>
</a>
<nav>
<a href="/blogs/">Blogs</a>

<a href="/posts/">GSoC</a>

<a href="/archive/">Archive</a>

<a href="/projects/">Projects</a>

</nav>
</header>
  <main>

<content>
  <h1 id="this-week">This week</h1>
<ul>
<li>Implemented the transformer model for captioning task</li>
<li>Presented the demo of the pipeline to the mentors</li>
</ul>
<h2 id="changing-the-datasettorchdataset-instance-ie">Changing the dataset(torch.Dataset instance i.e)</h2>
<p>Earlier the dataset instance of PyTorch that I was using was simple. Whenever I referred an index it would return the image and numericalized version of the caption. But I wanted to use GPT2 for what it is good for. Hint: Text generation. So I changed the captions by providing a prompt as a token. A prompt would be really simple like &ldquo;What is happening in the painting&rdquo; or &ldquo;Who is in the painting&rdquo; and so the idea is that by passing these prompts the model will look at the image and generate a response describing the painting. I felt this is similar to how we function and hence I added it.</p>
<p>These are some of the prompts I used.</p>
<pre tabindex="0"><code>Describe the painting.
What’s going on in this artwork?
What title would you give this artwork?
What symbols do you notice in the artwork?
What is the subject matter
Describe the agents in painting
Describe the icons in painting
Best title for this painting
Caption this painting
Describe the artwork
What is this painting about
&quot; &quot;
</code></pre><p>Notice I have used an empty prompt as well. That is because the model should be able to function just about fine even if we don&rsquo;t pass a prompt</p>
<h2 id="implementing-the-transformer">Implementing the Transformer</h2>
<p><img src="/final_model.png" alt="final_model">
Over the past few blogs we have seen this model architecture a lot. This week was the one when I finally implemented. As discussed in last week&rsquo;s blog I have changed the feature extractor from FRCNN to ViT and the decoder inputs from a Word2Vec+LSTM model to GPT2 based one. I have also added stacks of encoder and decoder blocks to these inputs and finally fused the modality by using the outputs from the encoder block and used them as inputs in decoder block as well. This way we have image-to-image, text-to-text and image-to-text interactions thus making the model multimodal.</p>
<h2 id="meetingdemo">Meeting+Demo</h2>
<p>In this week&rsquo;s meeting I was supposed to show the demo of the pipeline. After explaining the working of my model I showcased the demo on a test image shared by one of my mentors Marcelo. The painting was of Jesus hanging on a cross and surrounded by Mary and another Saint. The model I had built was able to successfully describe some of the agents in the painting.</p>
<p>Next my mentors asked what do I propose to do in the remaining days of this programs. I proposed that with certain improvements in the captions I had collected, we could improve the performance even more. Finally I also said that I would make this pipeline work on Case HPC which would subsequently mean the end of this project (at least for the Google Summer of Code)</p>
<h1 id="next-week">Next week</h1>
<ul>
<li>
<p>So the plan for next week is to first deploy the pipeline to Case HPC and then work on improving caption quality. While there are still some things I would love to work on. I can only intend to work on them once the initial deliverables are completed.</p>
</li>
<li>
<p>While the model is built I have to properly evaluate it by using different captioning metrics like BLEU, METEOR, CIDER etc in order to prove the usefulness of the model. Hence I will work on an inference/testing module for the same.</p>
</li>
</ul>

</content>



<p>
  
</p>

  </main>
  <footer>Made with <a href="https://github.com/janraasch/hugo-bearblog/">Hugo ʕ•ᴥ•ʔ Bear</a>
</footer>

  
</body>

</html>
