<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://lazyCodes7.github.io/images/favicon.png" />
<title>Week-14 (Coding Period) 18th August - 25th August | Rishab Mudliar</title>
<meta name="title" content="Week-14 (Coding Period) 18th August - 25th August" />
<meta name="description" content="This week  Used a transformer based model to work on the captioning task  A Transformer that uses images &#43; text During the last week&rsquo;s meeting I had concluded that using models like Word2Vec might not always work as they have a fixed representation of each word but words can differ contextually so this week I decided to use some better language models like GPT2 and BERT.
VisionEncoderDecoder For incorporating transformer I used huggingface&rsquo;s VisionEncoderDecoder implementation which helps us in initializing any vision-transformer(ViT, BEiT etc) based model as the encoder and a language model(GPT2, BERT, RoBERTa etc) as the decoder." />
<meta name="keywords" content="" />


<meta property="og:title" content="Week-14 (Coding Period) 18th August - 25th August" />
<meta property="og:description" content="This week  Used a transformer based model to work on the captioning task  A Transformer that uses images &#43; text During the last week&rsquo;s meeting I had concluded that using models like Word2Vec might not always work as they have a fixed representation of each word but words can differ contextually so this week I decided to use some better language models like GPT2 and BERT.
VisionEncoderDecoder For incorporating transformer I used huggingface&rsquo;s VisionEncoderDecoder implementation which helps us in initializing any vision-transformer(ViT, BEiT etc) based model as the encoder and a language model(GPT2, BERT, RoBERTa etc) as the decoder." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazyCodes7.github.io/posts/week14/" /><meta property="og:image" content="https://lazyCodes7.github.io/images/share.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-25T23:56:36+05:30" />
<meta property="article:modified_time" content="2022-08-25T23:56:36+05:30" /><meta property="og:site_name" content="Rishab M" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lazyCodes7.github.io/images/share.png"/>

<meta name="twitter:title" content="Week-14 (Coding Period) 18th August - 25th August"/>
<meta name="twitter:description" content="This week  Used a transformer based model to work on the captioning task  A Transformer that uses images &#43; text During the last week&rsquo;s meeting I had concluded that using models like Word2Vec might not always work as they have a fixed representation of each word but words can differ contextually so this week I decided to use some better language models like GPT2 and BERT.
VisionEncoderDecoder For incorporating transformer I used huggingface&rsquo;s VisionEncoderDecoder implementation which helps us in initializing any vision-transformer(ViT, BEiT etc) based model as the encoder and a language model(GPT2, BERT, RoBERTa etc) as the decoder."/>



<meta itemprop="name" content="Week-14 (Coding Period) 18th August - 25th August">
<meta itemprop="description" content="This week  Used a transformer based model to work on the captioning task  A Transformer that uses images &#43; text During the last week&rsquo;s meeting I had concluded that using models like Word2Vec might not always work as they have a fixed representation of each word but words can differ contextually so this week I decided to use some better language models like GPT2 and BERT.
VisionEncoderDecoder For incorporating transformer I used huggingface&rsquo;s VisionEncoderDecoder implementation which helps us in initializing any vision-transformer(ViT, BEiT etc) based model as the encoder and a language model(GPT2, BERT, RoBERTa etc) as the decoder."><meta itemprop="datePublished" content="2022-08-25T23:56:36+05:30" />
<meta itemprop="dateModified" content="2022-08-25T23:56:36+05:30" />
<meta itemprop="wordCount" content="547"><meta itemprop="image" content="https://lazyCodes7.github.io/images/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Rishab Mudliar</h2>
</a>
<nav>
<a href="/blogs/">Blogs</a>

<a href="/posts/">GSoC</a>

<a href="/projects/">Projects</a>

<a href="/archive/">Archive</a>

<a href="/cv/cv.pdf">CV</a>

</nav>
</header>
  <main>

<content>
  <h1 id="this-week">This week</h1>
<ul>
<li>Used a transformer based model to work on the captioning task</li>
</ul>
<h2 id="a-transformer-that-uses-images--text">A Transformer that uses images + text</h2>
<p>During the last week&rsquo;s meeting I had concluded that using models like Word2Vec might not always work as they have a fixed representation of each word but words can differ contextually so this week I decided to use some better language models like GPT2 and BERT.</p>
<h3 id="visionencoderdecoder">VisionEncoderDecoder</h3>
<p>For incorporating transformer I used huggingface&rsquo;s VisionEncoderDecoder <a href="https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder">implementation</a> which helps us in initializing any vision-transformer(ViT, BEiT etc) based model as the encoder and a language model(GPT2, BERT, RoBERTa etc) as the decoder.</p>
<h3 id="using-a-vit-and-not-an-object-detector">Using a ViT and not an object-detector</h3>
<p>While using ViT as a feature-extractor in encoder is good but in the past few weeks I had also worked on an object-detection module to do the same task. So why change now?</p>
<p><img src="/objvsvit.png" alt="vitvsfrcnn"></p>
<p>The answer lies in the image. Normally image-captioning models that use object-detection require another dataset that has if not all then at least some objects that will be used in the dataset for captioning. Now this method is fine if we are working with captioning of a real-life object as we have well-harvested datasets like MS-COCO or Visual Genome. But with respect to art there doesn&rsquo;t exist much. In fact w.r.t to Christian Iconography there is only one dataset that tackles object-detection and that is also not for all the vast amount of saints we know of. In such a case using ViT can be really beneficial. By using a Vision Transformer we can convert image into set of tokens/patches. By doing this we are basically viewing each and every corner of our image unlike an object-detection model that normally uses a CNN backbone. Next similar to an object-detection model we can assign importance to these patches by using the concept of attention. This way we discard any useless information in the image and since we can finetune this model we can improve the performance of it while training of course. Hence, due to the mentioned points I decided to try out ViT as a feature extractor</p>
<p>Image taken from this <a href="https://arxiv.org/pdf/2201.12723.pdf">paper</a></p>
<h3 id="results">Results</h3>
<p>So by following the methodology I used ViT as my encoder followed by using GPT2/BERT as my decoder. I padded the texts to max_length batch wise and started training. This is where things got interesting. Somehow the loss kept on reducing to really low values in the range of 0.000455. This meant that the model was fantastic at doing what it was supposed to do but turns out it was not the case. During inference all the model was doing was predicting <!-- raw HTML omitted --> i.e end of sentence tokens. I tried doing lot of things to tackle this but somehow it was not working.</p>
<h1 id="next-week">Next week</h1>
<p>This brings me to the plan of action for next week. Somehow the vision_encoder_decoder of huggingface was not working out for me and since there was not much documentation for the same, I decided to implement a transformer from scratch and include ViT and GPT2 implementations from huggingface as add-ons to my own transformer. I felt this would be better as I would be able to control the things that I was not able to see in huggingface implementation and also understand the working of my model in a better way.</p>

</content>



<p>
  
</p>

  </main>
  <footer>
</footer>

  
</body>

</html>
