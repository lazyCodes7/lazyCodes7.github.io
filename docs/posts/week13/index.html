<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://lazyCodes7.github.io/images/favicon.png" />
<title>Week-13 (Coding Period) 11th August - 18th August | Rishab Mudliar</title>
<meta name="title" content="Week-13 (Coding Period) 11th August - 18th August" />
<meta name="description" content="This week  Worked on using the captions I had collected during the curation stage I also build up a CNN&#43;LSTM model to see if we could achieve good captioning results without use of Transformers  Approach towards using the text. A image captioning model makes use of both text and an image but interpreting text can be difficult. Normally to solve this we use word embeddings which give us an idea about the similarity between two or more words." />
<meta name="keywords" content="" />


<meta property="og:title" content="Week-13 (Coding Period) 11th August - 18th August" />
<meta property="og:description" content="This week  Worked on using the captions I had collected during the curation stage I also build up a CNN&#43;LSTM model to see if we could achieve good captioning results without use of Transformers  Approach towards using the text. A image captioning model makes use of both text and an image but interpreting text can be difficult. Normally to solve this we use word embeddings which give us an idea about the similarity between two or more words." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazyCodes7.github.io/posts/week13/" /><meta property="og:image" content="https://lazyCodes7.github.io/images/share.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-18T23:56:36+05:30" />
<meta property="article:modified_time" content="2022-08-18T23:56:36+05:30" /><meta property="og:site_name" content="Rishab M" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lazyCodes7.github.io/images/share.png"/>

<meta name="twitter:title" content="Week-13 (Coding Period) 11th August - 18th August"/>
<meta name="twitter:description" content="This week  Worked on using the captions I had collected during the curation stage I also build up a CNN&#43;LSTM model to see if we could achieve good captioning results without use of Transformers  Approach towards using the text. A image captioning model makes use of both text and an image but interpreting text can be difficult. Normally to solve this we use word embeddings which give us an idea about the similarity between two or more words."/>



<meta itemprop="name" content="Week-13 (Coding Period) 11th August - 18th August">
<meta itemprop="description" content="This week  Worked on using the captions I had collected during the curation stage I also build up a CNN&#43;LSTM model to see if we could achieve good captioning results without use of Transformers  Approach towards using the text. A image captioning model makes use of both text and an image but interpreting text can be difficult. Normally to solve this we use word embeddings which give us an idea about the similarity between two or more words."><meta itemprop="datePublished" content="2022-08-18T23:56:36+05:30" />
<meta itemprop="dateModified" content="2022-08-18T23:56:36+05:30" />
<meta itemprop="wordCount" content="531"><meta itemprop="image" content="https://lazyCodes7.github.io/images/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Rishab Mudliar</h2>
</a>
<nav>
<a href="/blogs/">Blogs</a>

<a href="/posts/">GSoC</a>

<a href="/archive/">Archive</a>

<a href="/projects/">Projects</a>

</nav>
</header>
  <main>

<content>
  <h1 id="this-week">This week</h1>
<ul>
<li>Worked on using the captions I had collected during the curation stage</li>
<li>I also build up a CNN+LSTM model to see if we could achieve good captioning results without use of Transformers</li>
</ul>
<h2 id="approach-towards-using-the-text">Approach towards using the text.</h2>
<p>A image captioning model makes use of both text and an image but interpreting text can be difficult. Normally to solve this we use word embeddings which give us an idea about the similarity between two or more words.</p>
<p>In this week I focussed on training a word embedding model that could learn to associate the various agents in our painting. To give an idea if we have Mary then we should words like Virgin or Christ appear more nearer to her as Christ is her son and Virgin is a title associated with her.</p>
<p>Here is the workflow for the same</p>
<p><img src="/4.png" alt=""></p>
<p>The idea is to train a Word2Vec model that will learn to interpret the text and give us some understandable representation of what we know about Christian Iconography. So for instance when I pass &lsquo;John The Baptist&rsquo; to my word2vec model it should predict some words like Mary, Christ being the closest words according to Christian Iconography.</p>
<p>Here are some preliminary results of the training. I have used gensim&rsquo;s word2vec implementation for training on the corpus</p>
<h3 id="1-virgin-mary">1. Virgin Mary</h3>
<p><img src="/6.png" alt=""></p>
<h3 id="2-saint-sebastian">2. Saint Sebastian</h3>
<p><img src="/7.png" alt=""></p>
<h3 id="3-representation-of-the-words-as-vectors">3. Representation of the words as vectors</h3>
<p><img src="/5.png" alt=""></p>
<h2 id="building-a-captioning-model">Building a captioning model</h2>
<p>Since past few years, captioning models generally use an encoder-decoder strategy where the encoder is a CNN and the decoder is an LSTM. Since I had implemented these parts I felt it would be good to test out this strategy and see if I could build something without using a Transformer.</p>
<p>The workflow of this model generated from the overall workflow of my captioning module</p>
<p><img src="/captioner.png" alt=""></p>
<p>I trained this model for 10 epochs and the loss reduced steadily till 0.4. Following are some of the captions generated using this strategy.</p>
<h3 id="example1">Example1</h3>
<p><img src="/11.png" alt=""></p>
<h3 id="example2">Example2</h3>
<p><img src="/12.png" alt=""></p>
<p>If we observe, the captions generated are far from perfect.</p>
<p>Another interesting thing happening with the model I trained.</p>
<p>For a lot of the images the caption is as following <code>the holy family with st john the baptist</code>. I am not sure why this was as the loss was constantly decreasing. I think it might have something to do with word2vec as it misses out on contextual understanding of words and tries to represent each word with a fixed vector.</p>
<p>To give an example if word <code>bank</code> has a certain representation in word2vec then two statements <code>river bank</code> and <code>bank robbery</code> might have almost same representation</p>
<h2 id="meeting">Meeting</h2>
<p>Based on last week&rsquo;s discussion I was supposed to show how I was planning to use text in my model. So I showcased the results of this blog. I got some suggestions regarding using language models that are trained on a large corpus (gpt2 for eg). I was also suggested to use sections from Emile Male&rsquo;s book as corpus to improve the performance of my model. Apart from that I was instructed to showcase a demo of the captioning process for next week</p>
<h1 id="next-week">Next week</h1>
<ul>
<li>Exploring some language models like GPT2, BeRT</li>
<li>Also finishing the captioning module for demo</li>
</ul>

</content>



<p>
  
</p>

  </main>
  <footer>
</footer>

  
</body>

</html>
