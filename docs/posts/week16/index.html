<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://lazyCodes7.github.io/images/favicon.png" />
<title>Week-16 (Coding Period) 2nd September - 11th September | Rishab Mudliar</title>
<meta name="title" content="Week-16 (Coding Period) 2nd September - 11th September" />
<meta name="description" content="This week  Deployed the pipeline to Case HPC Documented and commented the codebase  Current progress of the pipeline Stage 1: Curation After working on collecting paintings related to Christian Iconography for 3-4 weeks. The final version of the dataset consists of approximately 9.6k data points.
Museums  Web Gallery of Art - 5k images Art Institute of Chicago - 500-700 Met Art - 500-700 Corpus Viteraeum - 2k National Gallery of Art, Washington DC - 1k-2k  Artforms." />
<meta name="keywords" content="" />


<meta property="og:title" content="Week-16 (Coding Period) 2nd September - 11th September" />
<meta property="og:description" content="This week  Deployed the pipeline to Case HPC Documented and commented the codebase  Current progress of the pipeline Stage 1: Curation After working on collecting paintings related to Christian Iconography for 3-4 weeks. The final version of the dataset consists of approximately 9.6k data points.
Museums  Web Gallery of Art - 5k images Art Institute of Chicago - 500-700 Met Art - 500-700 Corpus Viteraeum - 2k National Gallery of Art, Washington DC - 1k-2k  Artforms." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lazyCodes7.github.io/posts/week16/" /><meta property="og:image" content="https://lazyCodes7.github.io/images/share.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-09T23:56:36+05:30" />
<meta property="article:modified_time" content="2022-09-09T23:56:36+05:30" /><meta property="og:site_name" content="Rishab M" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lazyCodes7.github.io/images/share.png"/>

<meta name="twitter:title" content="Week-16 (Coding Period) 2nd September - 11th September"/>
<meta name="twitter:description" content="This week  Deployed the pipeline to Case HPC Documented and commented the codebase  Current progress of the pipeline Stage 1: Curation After working on collecting paintings related to Christian Iconography for 3-4 weeks. The final version of the dataset consists of approximately 9.6k data points.
Museums  Web Gallery of Art - 5k images Art Institute of Chicago - 500-700 Met Art - 500-700 Corpus Viteraeum - 2k National Gallery of Art, Washington DC - 1k-2k  Artforms."/>



<meta itemprop="name" content="Week-16 (Coding Period) 2nd September - 11th September">
<meta itemprop="description" content="This week  Deployed the pipeline to Case HPC Documented and commented the codebase  Current progress of the pipeline Stage 1: Curation After working on collecting paintings related to Christian Iconography for 3-4 weeks. The final version of the dataset consists of approximately 9.6k data points.
Museums  Web Gallery of Art - 5k images Art Institute of Chicago - 500-700 Met Art - 500-700 Corpus Viteraeum - 2k National Gallery of Art, Washington DC - 1k-2k  Artforms."><meta itemprop="datePublished" content="2022-09-09T23:56:36+05:30" />
<meta itemprop="dateModified" content="2022-09-09T23:56:36+05:30" />
<meta itemprop="wordCount" content="1176"><meta itemprop="image" content="https://lazyCodes7.github.io/images/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Rishab Mudliar</h2>
</a>
<nav>
<a href="/blogs/">Blogs</a>

<a href="/posts/">GSoC</a>

<a href="/archive/">Archive</a>

<a href="/projects/">Projects</a>

</nav>
</header>
  <main>

<content>
  <h1 id="this-week">This week</h1>
<ul>
<li>Deployed the pipeline to Case HPC</li>
<li>Documented and commented the codebase</li>
</ul>
<h2 id="current-progress-of-the-pipeline">Current progress of the pipeline</h2>
<p><img src="/final_pipeline.png" alt=""></p>
<h3 id="stage-1-curation">Stage 1: Curation</h3>
<p>After working on collecting paintings related to Christian Iconography for 3-4 weeks. The final version of the dataset consists of approximately 9.6k data points.</p>
<h4 id="museums">Museums</h4>
<ol>
<li><a href="https://www.wga.hu/">Web Gallery of Art</a> - 5k images</li>
<li><a href="https://www.artic.edu/collection">Art Institute of Chicago</a> - 500-700</li>
<li><a href="https://www.metmuseum.org/">Met Art</a> - 500-700</li>
<li><a href="https://corpusvitrearum.de/">Corpus Viteraeum</a> - 2k</li>
<li><a href="https://www.nga.gov/">National Gallery of Art, Washington DC</a> - 1k-2k</li>
</ol>
<h4 id="artforms">Artforms.</h4>
<ol>
<li>Painting</li>
<li>Print</li>
<li>Stained glass</li>
</ol>
<h4 id="metadata">Metadata.</h4>
<ol>
<li>Title</li>
<li>Medium</li>
<li>Artist</li>
<li>Creation Time</li>
<li>Description(Contextual meaning basically)</li>
</ol>
<p>Note: The paintings itself are largely unfiltered.</p>
<p>Download the dataset and corresponding metadata <a href="https://drive.google.com/drive/folders/1a1vUdRYvFHzw3xs_Y1Yxfe_tanLogWBj?usp=sharing">here</a></p>
<p>You can also generate the dataset from scratch by using the curation module from my <a href="https://github.com/lazyCodes7/RedHenLab_Multimodal_Christian_Art_Tagging">repository</a></p>
<h3 id="stage-2-extracting-features">Stage 2: Extracting features.</h3>
<p>The Transformer model that I proposed requires a set of tokens/patches as input. In order to accomplish this I tried out two ways.</p>
<h4 id="using-frcnn">Using FRCNN</h4>
<p>The first method that I tried out was using an object-detection model for extracting certain ROIs in the image. For this I again worked on generating a dataset consisting of around 500-600 images and further increased to 1200 images by applying data augmentation.</p>
<p>Following are the classes employed.</p>
<ul>
<li>Angel</li>
<li>Arrow</li>
<li>Baby</li>
<li>Book</li>
<li>Christ</li>
<li>Cross</li>
<li>Crown</li>
<li>Cup</li>
<li>Flower</li>
<li>Fruit</li>
<li>Horse</li>
<li>Instrument</li>
<li>Key</li>
<li>Lamb</li>
<li>Lion</li>
<li>Mary</li>
<li>Pen</li>
<li>Saint</li>
<li>Skull</li>
<li>Staff</li>
<li>Sword</li>
</ul>
<h5 id="performance">Performance</h5>
<p>16.3%
mAP</p>
<p>71.8%
precision</p>
<p>14.3%
recall</p>
<h5 id="training-curves">Training curves</h5>
<p><img src="/results_frcnn.png" alt="results"></p>
<h5 id="examples">Examples</h5>
<p><img src="/frcnn_outputs.jpg" alt="example"></p>
<h4 id="using-vit">Using ViT</h4>
<p>One of the problems with using object-detection for feature extraction is that we need some datasets as a pre-requisite. While this is possible in case of real-life objects it is difficult when tackling paintings as there are not much datasets. Hence using ViT is a newer way to extract features. I have also mentioned why I have done this in my <a href="https://lazycodes7.github.io/posts/week14/">blog</a></p>
<h5 id="training-vit">Training ViT</h5>
<p>While we can use pretrained ViT trained on datasets like ImageNet. The performance could improve if we trained it on a dataset in similar domain. Hence I decided to train the ViT on the ArtDL Dataset</p>
<p>ArtDL dataset is a recent dataset related to classification of saints in the paintings. It has over 45k data points spanning over 19 classes.</p>
<p>My approach towards using the ArtDL dataset was to reduce the problem a more simpler one. Given we are looking at a painting can we know whether the painting has mary a male saint or a female saint in it. By using this approach I reduced the problem to a 3-class multi-classification problem.</p>
<h4 id="performance-1">Performance</h4>
<p>Epochs - 10</p>
<ul>
<li>Mary Accuracy - 75%</li>
<li>Female Saint Accuracy - 95%</li>
<li>Male Saint Accuracy - 95%</li>
</ul>
<p>Note - In the dataset the instances of Male and Female Saints are less and hence there is a class imbalance that needs to be fixed. Due to those reasons we can&rsquo;t really think of 95% accuracy in both cases as the actual result. I would have to do more work on these two classes to get robust results.</p>
<h3 id="stage-3---captioning">Stage 3 - Captioning</h3>
<h4 id="final-architecture">Final architecture</h4>
<p><img src="/final_model.png" alt="arch">
In the past few weeks I had tried out different language models and  feature extraction models. For the finally architecture I have fixated on using ViT as my feature extractor and GPT2 as my language model/decoder. In this the feature extraction module is pretrained on the ArtDL dataset while the GPT2 is finetuned. We can also train GPT2 individually on a corpus related to this topic but I have got considerably good results even without this and hence for now I haven&rsquo;t tried out this approach.</p>
<h4 id="performance-of-the-model">Performance of the model</h4>
<table>
<thead>
<tr>
<th>BLEU-1</th>
<th>BLEU-2</th>
<th>BLEU-3</th>
<th>BLEU-4</th>
<th>METEOR</th>
<th>ROUGE-L</th>
<th>CIDER</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.314</td>
<td>0.172</td>
<td>0.101</td>
<td>0.052</td>
<td>0.141</td>
<td>0.312</td>
<td>0.068</td>
</tr>
</tbody>
</table>
<p>Since we don&rsquo;t have other datasets or models on this. This can be thought of as the current-benchmark for this architecture. In future this will be updated accordingly as I try out more approaches towards captioning</p>
<h4 id="example">Example</h4>
<p>Here is an example that I had shown during the demo of my pipeline.</p>
<p><img src="/christ.jpg" alt="christ"></p>
<h5 id="using-prompts">Using prompts</h5>
<p>In order to generate captions the model I have built uses prompts. A prompt is a simple thing like &ldquo;What is in this painting&rdquo; , &ldquo;Caption this painting&rdquo;. Since GPT2 is very useful for text generation I have modified the dataset in a way that each title/caption has some random prompt associated with and so during inference we pass in the image and a random prompt to generate the caption.</p>
<p>So in context with the image I have used, these are the prompts and the results.</p>
<p>Describe the painting.</p>
<p>Answer - Christ on the Cross</p>
<p>What symbols do you notice in the artwork?</p>
<p>Answer - Virgin Mary</p>
<p>Describe the icons in the painting</p>
<p>Answer - Saint John</p>
<p>And this is the actual title that was not passed during inference
Christ on the Cross with the Virgin, Saint John the Evangelist, and Saint Catherine of Siena in Adoration</p>
<p>While the model is not absolutely perfect it is still able to generate some meaningful inference from the painting by using an image + prompt</p>
<p>And here are the defined prompts I used during training</p>
<pre tabindex="0"><code>Describe the painting.
What’s going on in this artwork?
What title would you give this artwork?
What symbols do you notice in the artwork?
What is the subject matter
Describe the agents in painting
Describe the icons in painting
Best title for this painting
Caption this painting
Describe the artwork
What is this painting about
&quot; &quot;
</code></pre><h2 id="future-plans">Future plans</h2>
<p>Looking back I feel like I have accomplished a lot of things in a domain that has not been explored much but since this is a new thing we can always do more.</p>
<ul>
<li>
<p>The first thing I want to continue work on is to generate more detailed descriptions of the paintings. So it should not just caption the agents in the paintings but should also describe properly what the agents might be doing and some story that we don&rsquo;t know of. This is a relatively difficult task to take I am still working on. The progress can be viewed in this notebook.</p>
</li>
<li>
<p>Next step is to leverage both images and text in the Emile Male Pipeline. While my captioning model uses both of these by fusing the modalities during training. We don&rsquo;t use these captions during inference and the only text we rely on are the prompts. In order to improve this I have tried to work on using the titles for generating captions that tell us about the finer details. This is what the first point also stresses upon.</p>
</li>
<li>
<p>Another idea I am thinking of is to build a search tool for enthusiasts in Christian Iconography. The idea is that the user will send in a title and the model could retrieve images similar to it. I feel that this would be a really useful thing for people just getting started with Iconography.</p>
</li>
</ul>
<h2 id="the-code">The code</h2>
<p>The results, implementation of the methods discussed can be found <a href="https://github.com/lazyCodes7/RedHenLab_Multimodal_Christian_Art_Tagging">here</a></p>
<h2 id="deploying-the-pipeline">Deploying the pipeline</h2>
<p>The pipeline is built in a modular way but they all combine together to form the Emile Male pipeline which is ready to be used on the Case HPC. Know more on how to do this in my <a href="https://github.com/lazyCodes7/RedHenLab_Multimodal_Christian_Art_Tagging#readme">README</a></p>
<ul>
<li></li>
</ul>

</content>



<p>
  
</p>

  </main>
  <footer>Made with <a href="https://github.com/janraasch/hugo-bearblog/">Hugo ʕ•ᴥ•ʔ Bear</a>
</footer>

  
</body>

</html>
